- name: Include global variables
  include_vars: ../../common/defaults/main.yml
- name: Include variables from Nexus
  include_vars: ../../nexus/defaults/main.yml
- name: Create storage directory
  file:
    path: "{{ ocs_storage_dir }}"
    state: directory
  delegate_to: "{{ hostvars[item]['kvm_host'] }}"
  with_items: "{{ groups['storage'] }}"
- systemd:
    name: libvirtd
    state: started
  delegate_to: "{{ hostvars[item]['kvm_host'] }}"
  with_items: "{{ groups['storage'] }}"
- name: Start Storage VM's
  virt:
    name: "{{ item }}"
    state: running
  delegate_to: "{{ hostvars[item]['kvm_host'] }}"
  with_items:
  - "{{ groups['storage'] }}"
- name: Start Storage VM's
  virt:
    name: "{{ item }}"
    command: status
  register: virt_startup_result
  until: virt_startup_result.status == 'running'
  retries: 12
  delay: 10
  failed_when:
  - virt_startup_result != 0
  - virt_startup_result.msg is defined and virt_startup_result.msg is not match(".*domain is running.*")
  delegate_to: "{{ hostvars[item]['kvm_host'] }}"
  loop: "{{ groups['storage'] }}"
- name: Stop Storage VM's
  virt:
    name: "{{ item }}"
    state: shutdown
  delegate_to: "{{ hostvars[item]['kvm_host'] }}"
  with_items:
  - "{{ groups['storage'] }}"
- name: Stop Storage VM's
  virt:
    name: "{{ item }}"
    command: status
  register: virt_shutdown_result
  until: virt_shutdown_result.status == 'shutdown'
  retries: 12
  delay: 10
  failed_when:
  - virt_shutdown_result != 0
  - virt_shutdown_result.msg is defined and virt_shutdown_result.msg is not match(".*domain is not running.*")
  delegate_to: "{{ hostvars[item]['kvm_host'] }}"
  loop: "{{ groups['storage'] }}"
- name: Create disk for Container Storage
  command: "qemu-img create -f raw {{ ocs_storage_dir }}/{{ item }}-ocs.raw {{ ocs_disk_size }}"
  args:
    creates: "{{ ocs_storage_dir }}/{{ item }}-ocs.raw"
  delegate_to: "{{ hostvars[item]['kvm_host'] }}"
  with_items: "{{ groups['storage'] }}"
- name: Attach Storage OSD disk to VM
  command: "virsh attach-disk {{ item }} {{ ocs_storage_dir }}/{{ item }}-ocs.raw vdb --serial ocsdisk-{{ item }} --cache none --persistent"
  register: attach_ocs_disk_result
  failed_when:
  - attach_ocs_disk_result.rc != 0
  - attach_ocs_disk_result.stderr_lines is not match(".*target vdb already exists.*")
  delegate_to: "{{ hostvars[item]['kvm_host'] }}"
  with_items: "{{ groups['storage'] }}"
- name: Stop Storage VM's
  virt:
    name: "{{ item }}"
    state: shutdown
  delegate_to: "{{ hostvars[item]['kvm_host'] }}"
  with_items:
  - "{{ groups['storage'] }}"
- name: Stop Storage VM's
  virt:
    name: "{{ item }}"
    command: status
  register: virt_shutdown_result
  until: virt_shutdown_result.status == 'shutdown'
  retries: 12
  delay: 10
  failed_when:
  - virt_shutdown_result != 0
  - virt_shutdown_result.msg is defined and virt_shutdown_result.msg is not match(".*domain is not running.*")
  delegate_to: "{{ hostvars[item]['kvm_host'] }}"
  loop: "{{ groups['storage'] }}"
- name: Set Max-Memory for Storage VM's
  shell: "virsh setmaxmem {{ item }} {{ ocs_memory_size }} --config"
  delegate_to: "{{ hostvars[item]['kvm_host'] }}"
  loop: "{{ groups['storage'] }}"
- name: Set Memory for Storage VM's
  shell: "virsh setmem {{ item }} {{ ocs_memory_size }} --config"
  delegate_to: "{{ hostvars[item]['kvm_host'] }}"
  loop: "{{ groups['storage'] }}"
- name: Start Storage VM's
  virt:
    name: "{{ item }}"
    state: running
  delegate_to: "{{ hostvars[item]['kvm_host'] }}"
  with_items:
  - "{{ groups['storage'] }}"
- name: Start Storage VM's
  virt:
    name: "{{ item }}"
    command: status
  register: virt_startup_result
  until: virt_startup_result.status == 'running'
  retries: 60
  delay: 10
  failed_when:
  - virt_startup_result != 0
  - virt_startup_result.msg is defined and virt_startup_result.msg is not match(".*domain is running.*")
  delegate_to: "{{ hostvars[item]['kvm_host'] }}"
  loop: "{{ groups['storage'] }}"
- name: Label Storage Nodes
  shell: "oc label node {{ item }}.{{ ocp_cluster_name }}.{{ocp_cluster_domain_name }} cluster.ocs.openshift.io/openshift-storage=\"\" --overwrite=true"
  register: label_node_result
  until: label_node_result.rc == 0
  retries: 60
  delay: 10
  with_items: "{{ groups['storage'] }}"
- name: Label Storage Nodes
  command: "oc label node {{ item }}.{{ ocp_cluster_name }}.{{ocp_cluster_domain_name }} topology.rook.io/rack=rack{{ rack_index }} --overwrite=true"
  retries: 60
  delay: 10
  loop: "{{ groups['storage'] }}"
  loop_control:
    index_var: rack_index
- name: Wait for storage nodes to be available
  shell: "oc get nodes -l cluster.ocs.openshift.io/openshift-storage= | grep Ready | wc -l"
  register: storage_nodes_available_result
  until: "{{ storage_nodes_available_result.stdout | int == groups['storage'] | length }}"
  retries: 60
  delay: 10
- name: Create local-storage project
  command: "oc new-project local-storage"
  register: create_local_storage_project_result
  until: create_local_storage_project_result.rc == 0 or create_local_storage_project_result.stderr is match(".*already exists.*")
  failed_when:
  - create_local_storage_project_result.rc != 0
  - create_local_storage_project_result.stderr is not match(".*already exists.*")
  retries: 60
  delay: 10
- name: Annotate local-storage project
  command: "oc annotate project local-storage openshift.io/node-selector='' --overwrite=true"
- name: Create Local Storage Operator-Group
  command: "oc create -f roles/ocs/files/local-storage-operator-group.yml -n local-storage"
  register: create_local_storage_operator_group_result
  failed_when:
  - create_local_storage_operator_group_result.rc != 0
  - '"already exists" not in create_local_storage_operator_group_result.stderr'
- name: Create Local Storage Operator-Subscription
  command: "oc create -f roles/ocs/files/local-storage-operator-subscription.yml"
  register: create_local_storage_operator_subscription_result
  failed_when:
  - create_local_storage_operator_subscription_result.rc != 0
  - '"already exists" not in create_local_storage_operator_subscription_result.stderr'
- name: Wait for Local Storage Operator Subscription to be available
  shell: "oc get sub local-storage-operator -o json -n local-storage | jq '.status.state' -r"
  register: local_storage_operator_subscription_state_result
  until: local_storage_operator_subscription_state_result.stdout is in ["AtLatestKnown","UpgradeAvailable"]
  retries: 30
  delay: 10
- name: Wait for Local Storage Operator to be available
  shell: "oc get deployment.apps/local-storage-operator -n local-storage -o json | jq '.status.readyReplicas'"
  register: local_storage_operator_deployment_replicas_result
  until: "(local_storage_operator_deployment_replicas_result.stdout | int) >= 1"
  retries: 30
  delay: 10
- name: Create local-storage template
  template:
    src: local-storage.yml.j2
    dest: /tmp/local-storage.yml
- name: Create local-storage class
  command: "oc create -f /tmp/local-storage.yml -n local-storage"
  register: create_local_storage_class_result
  failed_when:
  - create_local_storage_class_result.rc != 0
  - '"already exists" not in create_local_storage_class_result.stderr'
- name: Wait for Local Storage Daemon Sets to be available
  shell: "oc get daemonset.apps/{{ item }} -n local-storage -o json | jq '.status.numberReady'"
  register: local_storage_daemonsets_deployment_replicas_result
  with_items:
  - local-block-local-diskmaker
  - local-block-local-provisioner
  until: "(local_storage_daemonsets_deployment_replicas_result.stdout | int) >= 2"
  retries: 30
  delay: 10
- name: Create openshift-storage namespace
  command: "oc create namespace openshift-storage"
  register: create_openshift_storage_namespace_result
  failed_when:
  - create_openshift_storage_namespace_result.rc != 0
  - '"already exists" not in create_openshift_storage_namespace_result.stderr'
- name: Label operator-storage namespace
  command: "oc label namespace openshift-storage openshift.io/cluster-monitoring=true --overwrite"
- name: Create Operator-Group
  command: "oc create -f roles/ocs/files/cluster-storage-operator-group.yml -n openshift-storage"
  register: create_cluster_storage_operator_group_result
  failed_when:
  - create_cluster_storage_operator_group_result.rc != 0
  - '"already exists" not in create_cluster_storage_operator_group_result.stderr'
- name: Create Operator-Subscription
  command: "oc create -f roles/ocs/files/cluster-storage-operator-subscription.yml"
  register: create_cluster_storage_operator_subscription_result
  failed_when:
  - create_cluster_storage_operator_subscription_result.rc != 0
  - '"already exists" not in create_cluster_storage_operator_subscription_result.stderr'
- name: Wait for Storage Operator Subscription to be available
  shell: "oc get sub ocs-operator -o json -n openshift-storage | jq '.status.state' -r"
  register: cluster_storage_operator_subscription_state_result
  until: cluster_storage_operator_subscription_state_result.stdout is in ["UpgradeAvailable","AtLatestKnown"]
  retries: 120
  delay: 10
- name: Wait for OCS Operator to be available
  shell: "oc get deployment.apps/ocs-operator -n openshift-storage -o json | jq '.status.readyReplicas'"
  register: ocs_deployment_replicas_result
  until: "(ocs_deployment_replicas_result.stdout | int) >= 1"
  retries: 30
  delay: 10
- name: Wait for Rook Ceph Operator to be available
  shell: "oc get deployment.apps/rook-ceph-operator -n openshift-storage -o json | jq '.status.readyReplicas'"
  register: rook_ceph_deployment_replicas_result
  until: "(rook_ceph_deployment_replicas_result.stdout | int) >= 1"
  retries: 30
  delay: 10
- name: Wait for Noobaa Operator to be available
  shell: "oc get deployment.apps/noobaa-operator -n openshift-storage -o json | jq '.status.readyReplicas'"
  register: noobaa_deployment_replicas_result
  until: "(noobaa_deployment_replicas_result.stdout | int) >= 1"
  retries: 30
  delay: 10
- name: Create storage-cluster template
  template:
    src: cluster-storage.yml.j2
    dest: /tmp/cluster-storage.yml
- name: Create storage-cluster
  command: "oc create -f /tmp/cluster-storage.yml -n openshift-storage"
  register: create_cluster_storage_result
  failed_when:
  - create_cluster_storage_result.rc != 0
  - '"already exists" not in create_cluster_storage_result.stderr'
- name: Reduce resources on Noobaa Endpoint
  command: "oc patch deployment.apps/noobaa-endpoint -p '{\"spec\": {\"containers\": { \"limits\": { \"cpu\": \"250m\", \"memory\": \"250M\" }, \"requests\": { \"cpu\": \"250m\", \"memory\": \"250M\" }}}'"
  register: reduce_noobaa_resources_result
  until: reduce_noobaa_resources_result.rc == 0
  failed_when:
  - reduce_noobaa_resources_result.rc != 0
  retries: 120
  delay: 10
- name: Wait for OCS Operator to be available
  shell: "oc get deployment.apps/ocs-operator -n openshift-storage -o json | jq '.status.readyReplicas'"
  register: ocs_deployment_replicas_result
  until: "(ocs_deployment_replicas_result.stdout | int) >= 1"
  retries: 30
  delay: 10
- name: Make Ceph RBD the default storage-class
  command: "oc patch storageclass ocs-storagecluster-ceph-rbd -p '{\"metadata\": {\"annotations\": {\"storageclass.kubernetes.io/is-default-class\": \"true\"}}}'"
- name: Create monitoring config-map
  template:
    src: openshift-monitoring-config.yml.j2
    dest: /tmp/openshift-monitoring-config.yml
- name: Create monitoring config-map
  command: "oc create -f /tmp/openshift-monitoring-config.yml -n openshift-monitoring"
